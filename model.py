import numpy as np
import torch.nn as nn
from config import *

class Model():

    def __init__(self, latent_dim, dataloader, generator, discriminator, batch_size, lr_g, lr_d):
        
        self.generator = generator
        self.discriminator = discriminator
        self.batch_size = batch_size
        self.dataloader = dataloader
        
        self.criterion = nn.BCELoss()
        self.optim_g = torch.optim.Adam(self.generator.parameters(),
                            lr = lr_g, betas = (0.9, 0.999))
        
        self.optim_d = torch.optim.Adam(self.discriminator.parameters(),
                            lr = lr_d, betas = (0.9, 0.999))
        
        # Creating targets here rather than creating in each iteration
        self.target_ones = torch.ones((batch_size, 1), device=DEVICE)
        self.target_zeros = torch.zeros((batch_size, 1), device=DEVICE)

    
    def compute_accuracy(self, d_out, target):

        pred = torch.empty_like(d_out).copy_(d_out)
        pred[pred >= 0.5] = 1
        pred[pred < 0.5] = 0
        return (torch.sum(pred==target) /  target.shape[0]) * 100

    
    def generate_samples(self, latent_vec = None, num = None):

        num = self.batch_size if num is None else num
        latent_vec = GET_NOISE(num) if latent_vec is None else latent_vec

        with torch.no_grad():
            # eval mode to handle batch norm
            samples = self.generator(latent_vec)
        
        return samples.cpu()

    
    def plot_images(self, latent_vec = None, samples = None, num = 9, filename = "xyz.png"):

        if samples is None:
            samples = self.generate_samples(latent_vec = latent_vec, num = num) * -1    # multiply by -1 to invert background and foreground

        grid = torchvision.utils.make_grid(samples, nrow = NROW, normalize = True)
        plt.imshow(grid.numpy().transpose(1,2,0))
        plt.axis('off')
        plt.savefig(filename)
        plt.close()


    def train_generator_step(self, g_steps_per_iter=1):

        for step_idx in range(g_steps_per_iter):
            self.generator.zero_grad()

            latent_vec = GET_NOISE(self.batch_size)
            g_out = self.generator(latent_vec)
            d_out = self.discriminator(g_out) # Gradients flow back through the discriminator
            
            loss = self.criterion(d_out, self.target_ones)
            loss.backward()
            self.optim_g.step()

        # Returns float and not tensor [ Memory save by garbage collector ]
        return loss.item()
    

    def train_discriminator_step(self, real_samples, d_steps_per_iter=1):

        for step_idx in range(d_steps_per_iter):
            self.discriminator.zero_grad()

            # Discriminator should predict real images as 1
            d_out_real = self.discriminator(real_samples) 
            loss_real = self.criterion(d_out_real, self.target_ones)
            # acc_real = self.compute_accuracy(d_out_real, self.target_ones)

            # Discriminator should predict fake [generated by Gen.] as 0
            latent_vec = GET_NOISE(self.batch_size)
            with torch.no_grad():
                g_out = self.generator(latent_vec)
            
            d_out_fake = self.discriminator(g_out)
            loss_fake = self.criterion(d_out_fake, self.target_zeros)
            d_acc_on_gen = self.compute_accuracy(d_out_fake, self.target_zeros)

            # Joining computational graph for real and fake inpputs into one graph
            loss = (loss_real + loss_fake) / 2
            # acc = (acc_real + acc_fake) / 2

            loss.backward()
            self.optim_d.step()

        return loss_real.item(), loss_fake.item(), d_acc_on_gen.item()


    def train_epoch(self, print_freq, epoch):
        g_loss_epoch, real_d_loss_epoch, fake_d_loss_epoch, d_acc_epoch = 0,0,0,0

        for i, (real_samples, _) in enumerate(self.dataloader):

            real_samples = real_samples.to(DEVICE)
            loss_real, loss_fake, d_acc = self.train_discriminator_step(real_samples, D_STEPS_PER_ITER)

            real_d_loss_epoch += loss_real
            fake_d_loss_epoch += loss_fake
            d_acc_epoch       += d_acc

            loss_gen = self.train_generator_step(G_STEPS_PER_ITER)
            g_loss_epoch += loss_gen

            if print_freq and (i+1) % print_freq == 0:
                
                print(f"BATCH: {i+1}/{len(self.dataloader)}:\t"
                    f"Gen: {g_loss_epoch / (i+1):.3f}\t"
                    f"R_Dis: {real_d_loss_epoch / (i+1):.3f}\t"
                    f"F_Dis: {fake_d_loss_epoch / (i+1):.3f}\t",
                    f"Dis_acc: {d_acc_epoch / (i+1):.3f}\t",
                    )

                if BOOL_PLOT:
                    self.plot_images(num = PLOT_NUM,
                                 filename=PLOTS+f"Generated_EPOCH_{epoch+1}_BATCH_{i+1}.png")
                
        if print_freq:
            print()
        
        g_loss_epoch      /= len(self.dataloader)
        real_d_loss_epoch /= len(self.dataloader)
        fake_d_loss_epoch /= len(self.dataloader)
        d_acc_epoch      /= len(self.dataloader)

        return g_loss_epoch, (real_d_loss_epoch, fake_d_loss_epoch), d_acc_epoch
            


            

